# Mobile-Edge-LLM

A fully autonomous agent that enables **natural language-driven control** of **Android apps**, without accessing their source code or using any SDK. The system converts **user prompts** like â€œBook a cabâ€ or â€œSend a messageâ€ into actionable UI interactions (tap, scroll, type) by visually analyzing the phone screen in real time.

---

## ğŸ§© Key Features

- ğŸ§  **LLM-Powered Intent Understanding** using **Gemini API**
- ğŸ“¸ **Android Screen Capture** via **ADB + scrcpy**
- ğŸ§® **UI Element Segmentation** using Meta's **GroundingDINO**
- ğŸ§­ **Touch, Scroll, Input Simulation** to complete tasks
- ğŸ“Š **SQLite** database for session and message tracking
- ğŸ§± **FastAPI WebSocket Backend** with live interaction sessions

---

## ğŸ“‚ Project Structure

```
ğŸ“ root/
 â”£ ğŸ“œ main.py              â† Core logic: screen capture, UI segmentation, action execution
 â”£ ğŸ“œ models.py            â† SQLModel ORM definitions (ChatSession, Message)
 â”£ ğŸ“œ utils.py             â† Database utilities
 â”£ ğŸ“œ req.txt              â† Python dependencies
 â”£ ğŸ“œ .gitignore
 â”— ğŸ“ screenshots/         â† Saved screen captures with bounding boxes
```

---

## ğŸ”„ Workflow Overview

### 1. ğŸ§¾ Prompt Input
- User inputs a natural language prompt (e.g., "Order food on Zomato").
- Processed by Gemini (via Google Generative AI) to extract **goal**, **actions**, and **target app**.

### 2. ğŸ–¼ï¸ Screen Capture
- Android Debug Bridge (ADB) captures current screen.
- Image is preprocessed (grayscale, blur) via OpenCV.

### 3. ğŸ§  UI Element Segmentation
- GroundingDINO (Zero-shot) detects and classifies UI elements.
- Outputs bounding boxes with semantic meaning (buttons, fields, toggles, etc.).

### 4. ğŸ§· Token Mapping & LLM Plan
- Each UI element is converted to a **semantic token**.
- LLM uses these tokens to decide next action (e.g., `tap_index`, `input_text`, `swipe_from_center`).

### 5. âœ‹ UI Interaction Execution
- Commands are executed on the device via ADB:
  - Tap: `input tap x y`
  - Text Input: `input text`
  - Scroll: `swipe`
- Context-aware feedback loop updates based on live screen state.

### 6. ğŸ—‚ï¸ Logging
- All sessions and interactions are tracked in a local SQLite database (`chat.db`).

---

## ğŸ§  Technologies Used

| Component        | Technology                             |
|------------------|----------------------------------------|
| LLM              | Gemini API                             |
| UI Detection     | GroundingDINO (Zero-shot learning)     |
| Computer Vision  | OpenCV                                 |
| Screen Capture   | Android Debug Bridge (ADB)             |
| Automation       | Python subprocess (shell-based)        |
| Backend          | FastAPI (REST & WebSocket)             |
| Database         | SQLite + SQLModel ORM                  |

---

## ğŸ’¡ Example Prompts

- â€œSend â€˜On my wayâ€™ on WhatsApp to Momâ€
- â€œBook a cab to the airport on Olaâ€
- â€œSearch for dosa in Zomato and place an orderâ€
- â€œPlay Arijit Singhâ€™s playlist on Spotifyâ€

---

## ğŸ“¸ Sample Output

> Annotated screenshots with UI element bounding boxes are saved after every interaction.

![Example Screenshot](output_raw_20250515_111220.jpg)
![Example Screenshot](output_raw_20250515_111233.jpg)
![Example Screenshot](output_raw_20250515_111246.jpg)

---

## ğŸ“… Future Enhancements

- ğŸ”ˆ Add voice command input
- ğŸŒ Offline LLM integration for on-device inference
- ğŸ“Œ OCR layer for better UI context extraction
- ğŸ§© Cross-app task chaining

---

